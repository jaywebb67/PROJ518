{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8696,
     "status": "ok",
     "timestamp": 1756167706885,
     "user": {
      "displayName": "jay webb",
      "userId": "02822891855666551694"
     },
     "user_tz": -60
    },
    "id": "ed14lVZ0nqTI",
    "outputId": "582208bc-bf11-443c-8371-5afe4c0c8eb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_metric_learning\n",
      "  Downloading pytorch_metric_learning-2.9.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pytorch_metric_learning) (2.0.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from pytorch_metric_learning) (1.6.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from pytorch_metric_learning) (2.8.0+cu126)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from pytorch_metric_learning) (4.67.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (4.14.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->pytorch_metric_learning) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->pytorch_metric_learning) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->pytorch_metric_learning) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.6.0->pytorch_metric_learning) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.6.0->pytorch_metric_learning) (3.0.2)\n",
      "Downloading pytorch_metric_learning-2.9.0-py3-none-any.whl (127 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.8/127.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytorch_metric_learning\n",
      "Successfully installed pytorch_metric_learning-2.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_metric_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jgVugrIkn6wh"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import json, random, math, matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from multiprocessing import freeze_support\n",
    "from pytorch_metric_learning.losses import SupConLoss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class EEGEncoderCNN(nn.Module):\n",
    "    def __init__(self, n_classes, time_window=100, n_channels=62, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,  64,  (3,15), padding=(1,7), bias=False)\n",
    "        self.conv2 = nn.Conv2d(64, 128, (3,15), padding=(1,7), bias=False)\n",
    "        self.conv3 = nn.Conv2d(128,128, (3,15), padding=(1,7), bias=False)\n",
    "        self.conv4 = nn.Conv2d(128,256, (n_channels,3), padding=(0,1), bias=False)\n",
    "        self.pool1, self.pool2 = nn.MaxPool2d((1,2)), nn.MaxPool2d((1,2))\n",
    "        self.res_conv = nn.Conv2d(256,256,(1,3),padding=(0,1),bias=False)\n",
    "        self.norm_res = nn.GroupNorm(32,256)\n",
    "\n",
    "        T_after = time_window//4\n",
    "        self.time_attn = nn.MultiheadAttention(embed_dim=256, num_heads=4, batch_first=True)\n",
    "\n",
    "        self.adapt = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc_pre = nn.Linear(256,512)\n",
    "        self.fc_mu, self.fc_logvar = nn.Linear(512,hidden_dim), nn.Linear(512,hidden_dim)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim,512), nn.GELU(), nn.LayerNorm(512), nn.Dropout(0.5),\n",
    "            nn.Linear(512,n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.gelu(self.conv1(x)); x = F.gelu(self.conv2(x)); x = self.pool1(x)\n",
    "        x = F.gelu(self.conv3(x)); x = self.pool2(x); x = F.gelu(self.conv4(x))\n",
    "        r = x\n",
    "        x = F.gelu(self.norm_res(self.res_conv(x))) + r      # residual\n",
    "        B,C,_,T = x.shape\n",
    "        x = x.squeeze(2).permute(0,2,1)                      # (B,T,C)\n",
    "        x,_ = self.time_attn(x,x,x); x = x.permute(0,2,1).unsqueeze(2)\n",
    "        h = self.adapt(x).flatten(1)\n",
    "        h = F.gelu(self.fc_pre(h))\n",
    "        mu, logvar = self.fc_mu(h), self.fc_logvar(h)\n",
    "        z = mu + torch.randn_like(mu)*torch.exp(0.5*logvar)\n",
    "        return z, mu, logvar, self.classifier(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 668213,
     "status": "ok",
     "timestamp": 1756168465172,
     "user": {
      "displayName": "jay webb",
      "userId": "02822891855666551694"
     },
     "user_tz": -60
    },
    "id": "trN1jftCn9pT",
    "outputId": "0da38425-84bb-4a99-e8ab-89485fca07b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 001 | Loss: 2.2183 Acc: 0.1316 CE: 2.2183  KL: 6.4349  SupCon: 5.5674  \n",
      "Val Epoch 001 | Loss: 2.0682 Acc: 0.1800 CE: 2.0682  KL: 6.5906  SupCon: 5.5902  \n",
      "Train Epoch 002 | Loss: 2.1374 Acc: 0.1583 CE: 2.1374  KL: 7.2903  SupCon: 5.7148  \n",
      "Val Epoch 002 | Loss: 2.0216 Acc: 0.2040 CE: 2.0216  KL: 9.5137  SupCon: 6.1657  \n",
      "Train Epoch 003 | Loss: 2.0139 Acc: 0.2258 CE: 2.0139  KL: 14.2442  SupCon: 6.4645  \n",
      "Val Epoch 003 | Loss: 1.8562 Acc: 0.2880 CE: 1.8562  KL: 23.0837  SupCon: 7.1823  \n",
      "Train Epoch 004 | Loss: 1.8473 Acc: 0.3241 CE: 1.8473  KL: 23.5887  SupCon: 7.0309  \n",
      "Val Epoch 004 | Loss: 1.6073 Acc: 0.4520 CE: 1.6073  KL: 34.8137  SupCon: 7.4455  \n",
      "Train Epoch 005 | Loss: 1.6187 Acc: 0.4701 CE: 1.6187  KL: 23.5490  SupCon: 7.1509  \n",
      "Val Epoch 005 | Loss: 1.3092 Acc: 0.6440 CE: 1.3092  KL: 42.6493  SupCon: 7.2420  \n",
      "Train Epoch 006 | Loss: 1.4038 Acc: 0.5947 CE: 1.4038  KL: 24.4456  SupCon: 6.7575  \n",
      "Val Epoch 006 | Loss: 1.1211 Acc: 0.7400 CE: 1.1211  KL: 35.7764  SupCon: 6.3189  \n",
      "Train Epoch 007 | Loss: 1.2385 Acc: 0.6869 CE: 1.2385  KL: 24.7127  SupCon: 6.3144  \n",
      "Val Epoch 007 | Loss: 1.0581 Acc: 0.7680 CE: 1.0581  KL: 41.0712  SupCon: 6.0248  \n",
      "Train Epoch 008 | Loss: 1.1534 Acc: 0.7365 CE: 1.1534  KL: 27.4382  SupCon: 5.9724  \n",
      "Val Epoch 008 | Loss: 1.0061 Acc: 0.7880 CE: 1.0061  KL: 46.0961  SupCon: 5.6155  \n",
      "Train Epoch 009 | Loss: 1.0847 Acc: 0.7745 CE: 1.0847  KL: 27.4538  SupCon: 5.6606  \n",
      "Val Epoch 009 | Loss: 0.9586 Acc: 0.8560 CE: 0.9586  KL: 50.4681  SupCon: 5.4415  \n",
      "Train Epoch 010 | Loss: 1.0003 Acc: 0.8199 CE: 1.0003  KL: 31.7155  SupCon: 5.3799  \n",
      "Val Epoch 010 | Loss: 0.8910 Acc: 0.8680 CE: 0.8910  KL: 54.3227  SupCon: 5.1230  \n",
      "Train Epoch 011 | Loss: 0.9794 Acc: 0.8344 CE: 0.9791  KL: 29.5105  SupCon: 5.2632  \n",
      "Val Epoch 011 | Loss: 0.8988 Acc: 0.8520 CE: 0.8983  KL: 48.6507  SupCon: 5.0529  \n",
      "Train Epoch 012 | Loss: 0.9694 Acc: 0.8404 CE: 0.9688  KL: 31.0677  SupCon: 5.2054  \n",
      "Val Epoch 012 | Loss: 0.8891 Acc: 0.8640 CE: 0.8880  KL: 52.2223  SupCon: 4.9958  \n",
      "Train Epoch 013 | Loss: 0.9596 Acc: 0.8460 CE: 0.9587  KL: 30.0398  SupCon: 5.1737  \n",
      "Val Epoch 013 | Loss: 0.8770 Acc: 0.8720 CE: 0.8755  KL: 49.7300  SupCon: 4.9856  \n",
      "Train Epoch 014 | Loss: 0.9537 Acc: 0.8483 CE: 0.9525  KL: 30.3536  SupCon: 5.1597  \n",
      "Val Epoch 014 | Loss: 0.8933 Acc: 0.8560 CE: 0.8913  KL: 50.0789  SupCon: 5.0292  \n",
      "Train Epoch 015 | Loss: 0.9491 Acc: 0.8502 CE: 0.9475  KL: 30.5538  SupCon: 5.1887  \n",
      "Val Epoch 015 | Loss: 0.8859 Acc: 0.8800 CE: 0.8834  KL: 49.2132  SupCon: 5.0604  \n",
      "Train Epoch 016 | Loss: 0.9503 Acc: 0.8469 CE: 0.9486  KL: 27.9188  SupCon: 5.2323  \n",
      "Val Epoch 016 | Loss: 0.8968 Acc: 0.8680 CE: 0.8940  KL: 44.7048  SupCon: 5.0846  \n",
      "Train Epoch 017 | Loss: 0.9517 Acc: 0.8473 CE: 0.9496  KL: 27.9709  SupCon: 5.2173  \n",
      "Val Epoch 017 | Loss: 0.8837 Acc: 0.8680 CE: 0.8804  KL: 46.0084  SupCon: 5.0679  \n",
      "Train Epoch 018 | Loss: 0.9386 Acc: 0.8522 CE: 0.9364  KL: 27.0655  SupCon: 5.2097  \n",
      "Val Epoch 018 | Loss: 0.8869 Acc: 0.8680 CE: 0.8834  KL: 42.5993  SupCon: 5.1894  \n",
      "Train Epoch 019 | Loss: 0.9336 Acc: 0.8556 CE: 0.9313  KL: 25.3425  SupCon: 5.2228  \n",
      "Val Epoch 019 | Loss: 0.8897 Acc: 0.8640 CE: 0.8860  KL: 39.9903  SupCon: 5.1674  \n",
      "Train Epoch 020 | Loss: 0.9322 Acc: 0.8558 CE: 0.9297  KL: 24.7863  SupCon: 5.2470  \n",
      "Val Epoch 020 | Loss: 0.8752 Acc: 0.8760 CE: 0.8712  KL: 38.7344  SupCon: 5.1207  \n",
      "Train Epoch 021 | Loss: 0.9269 Acc: 0.8590 CE: 0.9242  KL: 24.2044  SupCon: 5.2337  \n",
      "Val Epoch 021 | Loss: 0.8822 Acc: 0.8680 CE: 0.8780  KL: 37.3571  SupCon: 5.1554  \n",
      "Train Epoch 022 | Loss: 0.9305 Acc: 0.8550 CE: 0.9276  KL: 23.1556  SupCon: 5.2800  \n",
      "Val Epoch 022 | Loss: 0.8808 Acc: 0.8720 CE: 0.8764  KL: 35.5782  SupCon: 5.2103  \n",
      "Train Epoch 023 | Loss: 0.9287 Acc: 0.8587 CE: 0.9257  KL: 21.7321  SupCon: 5.2883  \n",
      "Val Epoch 023 | Loss: 0.8836 Acc: 0.8720 CE: 0.8790  KL: 34.9545  SupCon: 5.2617  \n",
      "Train Epoch 024 | Loss: 0.9268 Acc: 0.8563 CE: 0.9237  KL: 21.4812  SupCon: 5.3439  \n",
      "Val Epoch 024 | Loss: 0.8797 Acc: 0.8800 CE: 0.8749  KL: 33.5782  SupCon: 5.2311  \n",
      "Train Epoch 025 | Loss: 0.9299 Acc: 0.8542 CE: 0.9267  KL: 20.8426  SupCon: 5.3422  \n",
      "Val Epoch 025 | Loss: 0.8696 Acc: 0.8920 CE: 0.8644  KL: 33.7875  SupCon: 5.2308  \n",
      "Train Epoch 026 | Loss: 0.9207 Acc: 0.8647 CE: 0.9173  KL: 20.0160  SupCon: 5.3391  \n",
      "Val Epoch 026 | Loss: 0.8735 Acc: 0.8840 CE: 0.8686  KL: 29.6914  SupCon: 5.3299  \n",
      "Train Epoch 027 | Loss: 0.9183 Acc: 0.8625 CE: 0.9149  KL: 18.9263  SupCon: 5.3403  \n",
      "Val Epoch 027 | Loss: 0.8900 Acc: 0.8680 CE: 0.8848  KL: 29.4949  SupCon: 5.4052  \n",
      "Train Epoch 028 | Loss: 0.9170 Acc: 0.8606 CE: 0.9136  KL: 18.0562  SupCon: 5.3440  \n",
      "Val Epoch 028 | Loss: 0.8631 Acc: 0.8720 CE: 0.8580  KL: 27.6303  SupCon: 5.3232  \n",
      "Train Epoch 029 | Loss: 0.9123 Acc: 0.8672 CE: 0.9087  KL: 18.3882  SupCon: 5.3520  \n",
      "Val Epoch 029 | Loss: 0.8886 Acc: 0.8680 CE: 0.8829  KL: 28.9556  SupCon: 5.4550  \n",
      "Train Epoch 030 | Loss: 0.9096 Acc: 0.8654 CE: 0.9058  KL: 18.0595  SupCon: 5.3431  \n",
      "Val Epoch 030 | Loss: 0.8532 Acc: 0.8880 CE: 0.8478  KL: 25.9441  SupCon: 5.3279  \n",
      "Train Epoch 031 | Loss: 0.9131 Acc: 0.8654 CE: 0.9094  KL: 16.9320  SupCon: 5.3899  \n",
      "Val Epoch 031 | Loss: 0.8414 Acc: 0.8880 CE: 0.8356  KL: 26.8209  SupCon: 5.3283  \n",
      "Train Epoch 032 | Loss: 0.9057 Acc: 0.8669 CE: 0.9018  KL: 16.9390  SupCon: 5.3844  \n",
      "Val Epoch 032 | Loss: 0.8867 Acc: 0.8680 CE: 0.8809  KL: 25.3973  SupCon: 5.4930  \n",
      "Train Epoch 033 | Loss: 0.9074 Acc: 0.8679 CE: 0.9034  KL: 16.3239  SupCon: 5.3584  \n",
      "Val Epoch 033 | Loss: 0.8607 Acc: 0.8720 CE: 0.8549  KL: 24.1194  SupCon: 5.3481  \n",
      "Train Epoch 034 | Loss: 0.9052 Acc: 0.8682 CE: 0.9011  KL: 16.2742  SupCon: 5.3562  \n",
      "Val Epoch 034 | Loss: 0.8559 Acc: 0.8840 CE: 0.8496  KL: 25.1153  SupCon: 5.3530  \n",
      "Train Epoch 035 | Loss: 0.9113 Acc: 0.8671 CE: 0.9071  KL: 15.8794  SupCon: 5.3936  \n",
      "Val Epoch 035 | Loss: 0.8521 Acc: 0.8760 CE: 0.8462  KL: 22.6964  SupCon: 5.3359  \n",
      "Train Epoch 036 | Loss: 0.9052 Acc: 0.8678 CE: 0.9009  KL: 15.3636  SupCon: 5.3784  \n",
      "Val Epoch 036 | Loss: 0.8472 Acc: 0.8800 CE: 0.8410  KL: 22.8576  SupCon: 5.3395  \n",
      "Train Epoch 037 | Loss: 0.9028 Acc: 0.8697 CE: 0.8986  KL: 14.5648  SupCon: 5.3867  \n",
      "Val Epoch 037 | Loss: 0.8648 Acc: 0.8800 CE: 0.8587  KL: 21.6547  SupCon: 5.4480  \n",
      "Train Epoch 038 | Loss: 0.9011 Acc: 0.8726 CE: 0.8968  KL: 14.3939  SupCon: 5.3681  \n",
      "Val Epoch 038 | Loss: 0.8751 Acc: 0.8640 CE: 0.8689  KL: 21.3233  SupCon: 5.4929  \n",
      "Train Epoch 039 | Loss: 0.9034 Acc: 0.8717 CE: 0.8989  KL: 14.5988  SupCon: 5.3801  \n",
      "Val Epoch 039 | Loss: 0.8610 Acc: 0.8880 CE: 0.8546  KL: 21.3479  SupCon: 5.3984  \n",
      "Train Epoch 040 | Loss: 0.9891 Acc: 0.8217 CE: 0.9842  KL: 15.4164  SupCon: 5.8069  \n",
      "Val Epoch 040 | Loss: 0.9210 Acc: 0.8560 CE: 0.9131  KL: 25.4683  SupCon: 5.6890  \n",
      "Train Epoch 041 | Loss: 0.9541 Acc: 0.8421 CE: 0.9488  KL: 16.9127  SupCon: 5.6512  \n",
      "Val Epoch 041 | Loss: 0.9262 Acc: 0.8520 CE: 0.9190  KL: 23.3366  SupCon: 5.7474  \n",
      "Train Epoch 042 | Loss: 0.9184 Acc: 0.8622 CE: 0.9130  KL: 17.1862  SupCon: 5.5123  \n",
      "Val Epoch 042 | Loss: 0.8505 Acc: 0.8880 CE: 0.8439  KL: 21.0989  SupCon: 5.2791  \n",
      "Train Epoch 043 | Loss: 0.8882 Acc: 0.8772 CE: 0.8833  KL: 15.4682  SupCon: 5.3017  \n",
      "Val Epoch 043 | Loss: 0.8879 Acc: 0.8680 CE: 0.8811  KL: 21.4757  SupCon: 5.5878  \n",
      "Train Epoch 044 | Loss: 0.8759 Acc: 0.8823 CE: 0.8709  KL: 15.9227  SupCon: 5.2515  \n",
      "Val Epoch 044 | Loss: 0.8352 Acc: 0.8920 CE: 0.8285  KL: 21.3489  SupCon: 5.2143  \n",
      "Train Epoch 045 | Loss: 0.8572 Acc: 0.8942 CE: 0.8524  KL: 14.8886  SupCon: 5.1287  \n",
      "Val Epoch 045 | Loss: 0.8868 Acc: 0.8840 CE: 0.8806  KL: 19.7244  SupCon: 5.4938  \n",
      "Train Epoch 046 | Loss: 0.8345 Acc: 0.9103 CE: 0.8300  KL: 14.4674  SupCon: 4.9991  \n",
      "Val Epoch 046 | Loss: 0.8534 Acc: 0.9160 CE: 0.8475  KL: 18.6897  SupCon: 5.1955  \n",
      "Train Epoch 047 | Loss: 0.8248 Acc: 0.9113 CE: 0.8201  KL: 14.5860  SupCon: 4.9179  \n",
      "Val Epoch 047 | Loss: 0.8403 Acc: 0.8920 CE: 0.8344  KL: 18.7231  SupCon: 5.1806  \n",
      "Train Epoch 048 | Loss: 0.8136 Acc: 0.9173 CE: 0.8092  KL: 13.8776  SupCon: 4.8456  \n",
      "Val Epoch 048 | Loss: 0.8794 Acc: 0.8840 CE: 0.8739  KL: 17.5897  SupCon: 5.5047  \n",
      "Train Epoch 049 | Loss: 0.7972 Acc: 0.9286 CE: 0.7929  KL: 13.3184  SupCon: 4.7495  \n",
      "Val Epoch 049 | Loss: 0.8496 Acc: 0.8840 CE: 0.8444  KL: 16.5310  SupCon: 5.2755  \n",
      "Train Epoch 050 | Loss: 0.7884 Acc: 0.9334 CE: 0.7842  KL: 13.3940  SupCon: 4.6835  \n",
      "Val Epoch 050 | Loss: 0.8293 Acc: 0.9120 CE: 0.8236  KL: 17.8708  SupCon: 5.1002  \n",
      "Train Epoch 051 | Loss: 0.7811 Acc: 0.9349 CE: 0.7768  KL: 13.5578  SupCon: 4.6130  \n",
      "Val Epoch 051 | Loss: 0.8543 Acc: 0.8960 CE: 0.8491  KL: 16.7298  SupCon: 5.1633  \n",
      "Train Epoch 052 | Loss: 0.7731 Acc: 0.9415 CE: 0.7689  KL: 13.1624  SupCon: 4.5535  \n",
      "Val Epoch 052 | Loss: 0.8450 Acc: 0.9080 CE: 0.8398  KL: 16.4520  SupCon: 5.2041  \n",
      "Train Epoch 053 | Loss: 0.7656 Acc: 0.9433 CE: 0.7615  KL: 13.0605  SupCon: 4.4767  \n",
      "Val Epoch 053 | Loss: 0.8445 Acc: 0.9080 CE: 0.8394  KL: 16.0041  SupCon: 5.2037  \n",
      "Train Epoch 054 | Loss: 0.7549 Acc: 0.9499 CE: 0.7509  KL: 12.7681  SupCon: 4.4097  \n",
      "Val Epoch 054 | Loss: 0.8555 Acc: 0.8880 CE: 0.8506  KL: 15.6030  SupCon: 5.2889  \n",
      "Train Epoch 055 | Loss: 0.7535 Acc: 0.9513 CE: 0.7494  KL: 13.0212  SupCon: 4.4132  \n",
      "Val Epoch 055 | Loss: 0.8709 Acc: 0.8880 CE: 0.8662  KL: 14.8143  SupCon: 5.3594  \n",
      "Train Epoch 056 | Loss: 0.8486 Acc: 0.5785 CE: 0.8445  KL: 13.0570  SupCon: 4.2710  \n",
      "Val Epoch 056 | Loss: 0.8186 Acc: 0.9120 CE: 0.8135  KL: 16.1101  SupCon: 4.9813  \n",
      "Train Epoch 057 | Loss: 0.8283 Acc: 0.6246 CE: 0.8242  KL: 12.7124  SupCon: 4.2547  \n",
      "Val Epoch 057 | Loss: 0.8804 Acc: 0.8800 CE: 0.8755  KL: 15.4559  SupCon: 5.3357  \n",
      "Train Epoch 058 | Loss: 0.8162 Acc: 0.4939 CE: 0.8123  KL: 12.1603  SupCon: 4.1952  \n",
      "Val Epoch 058 | Loss: 0.8958 Acc: 0.8600 CE: 0.8911  KL: 14.9459  SupCon: 5.4102  \n",
      "Train Epoch 059 | Loss: 0.8322 Acc: 0.6468 CE: 0.8283  KL: 12.2908  SupCon: 4.1453  \n",
      "Val Epoch 059 | Loss: 0.8820 Acc: 0.8800 CE: 0.8776  KL: 13.8811  SupCon: 5.3544  \n",
      "Train Epoch 060 | Loss: 0.7949 Acc: 0.5436 CE: 0.7912  KL: 11.7529  SupCon: 4.1002  \n",
      "Val Epoch 060 | Loss: 0.8439 Acc: 0.9120 CE: 0.8395  KL: 13.8369  SupCon: 5.1065  \n",
      "Train Epoch 061 | Loss: 0.8104 Acc: 0.4208 CE: 0.8067  KL: 11.4806  SupCon: 4.0768  \n",
      "Val Epoch 061 | Loss: 0.8484 Acc: 0.9000 CE: 0.8438  KL: 14.5350  SupCon: 5.1212  \n",
      "Train Epoch 062 | Loss: 0.8697 Acc: 0.5167 CE: 0.8660  KL: 11.6500  SupCon: 4.0940  \n",
      "Val Epoch 062 | Loss: 0.8537 Acc: 0.8880 CE: 0.8492  KL: 14.1586  SupCon: 5.1177  \n",
      "Train Epoch 063 | Loss: 0.8056 Acc: 0.5658 CE: 0.8019  KL: 11.4608  SupCon: 4.0599  \n",
      "Val Epoch 063 | Loss: 0.8328 Acc: 0.9080 CE: 0.8288  KL: 12.5919  SupCon: 5.0302  \n",
      "Train Epoch 064 | Loss: 0.8180 Acc: 0.6199 CE: 0.8144  KL: 11.3211  SupCon: 3.9803  \n",
      "Val Epoch 064 | Loss: 0.8448 Acc: 0.9080 CE: 0.8409  KL: 12.1660  SupCon: 5.0575  \n",
      "Train Epoch 065 | Loss: 0.8327 Acc: 0.6088 CE: 0.8290  KL: 11.6635  SupCon: 4.0025  \n",
      "Val Epoch 065 | Loss: 0.8632 Acc: 0.9000 CE: 0.8593  KL: 12.1277  SupCon: 5.2347  \n",
      "Train Epoch 066 | Loss: 0.8139 Acc: 0.4709 CE: 0.8103  KL: 11.3688  SupCon: 3.9429  \n",
      "Val Epoch 066 | Loss: 0.8483 Acc: 0.8960 CE: 0.8442  KL: 12.7148  SupCon: 5.0986  \n",
      "Train Epoch 067 | Loss: 0.7678 Acc: 0.5834 CE: 0.7643  KL: 11.0322  SupCon: 3.9265  \n",
      "Val Epoch 067 | Loss: 0.8723 Acc: 0.9040 CE: 0.8685  KL: 11.7092  SupCon: 5.2829  \n",
      "Train Epoch 068 | Loss: 0.8215 Acc: 0.5113 CE: 0.8180  KL: 10.9591  SupCon: 3.9386  \n",
      "Val Epoch 068 | Loss: 0.8478 Acc: 0.9040 CE: 0.8441  KL: 11.6346  SupCon: 5.0951  \n",
      "Train Epoch 069 | Loss: 0.7880 Acc: 0.6191 CE: 0.7846  KL: 10.9302  SupCon: 3.9326  \n",
      "Val Epoch 069 | Loss: 0.8854 Acc: 0.8880 CE: 0.8815  KL: 12.0110  SupCon: 5.3012  \n",
      "No improvement in 15 epochs, stopping early.\n",
      "Test Acc: 0.8449\n",
      "Confusion matrix, without normalization\n",
      "[[1371    1   11   11   17    8   16    5]\n",
      " [  17 1351   62   16   10   13   19   48]\n",
      " [  41   19 1214   13    6   18   38   61]\n",
      " [  58    8   28 1318   14   44   19   15]\n",
      " [  13   14   15    5 1411    5   28   13]\n",
      " [  36   14   26   26    3 1351   46   34]\n",
      " [  14    0   10    7    6   15 1351    7]\n",
      " [   9   11    8   25    5    3   11 1464]]\n",
      "Dumped train: latents (11876, 128), labels (11876,), images (11876,), subjects (11876,), CM at /content/drive/MyDrive/ImageNet_Images/preprocessed_splits/granularity/Time/all_channels/fine0/train_confusion.png\n",
      "Confusion matrix, without normalization\n",
      "[[28  0  0  1  1  0  0  0]\n",
      " [ 0 30  2  0  0  0  0  0]\n",
      " [ 0  0 22  0  1  0  3  4]\n",
      " [ 2  0  0 27  2  1  0  0]\n",
      " [ 0  0  2  0 30  0  0  0]\n",
      " [ 0  0  0  0  0 30  2  0]\n",
      " [ 0  0  0  0  0  0 30  0]\n",
      " [ 0  0  0  0  0  0  0 32]]\n",
      "Dumped val: latents (250, 128), labels (250,), images (250,), subjects (250,), CM at /content/drive/MyDrive/ImageNet_Images/preprocessed_splits/granularity/Time/all_channels/fine0/val_confusion.png\n",
      "Confusion matrix, without normalization\n",
      "[[26  0  1  1  1  1  0  0]\n",
      " [ 0 23  7  0  1  0  0  1]\n",
      " [ 1  1 47  1  1  3  5  1]\n",
      " [ 1  1  3 55  1  2  0  1]\n",
      " [ 1  3  3  0 54  1  2  0]\n",
      " [ 0  0  1  1  0 28  2  0]\n",
      " [ 3  0  2  2  0  0 53  0]\n",
      " [ 0  0  0  1  1  0  0 30]]\n",
      "Dumped test: latents (374, 128), labels (374,), images (374,), subjects (374,), CM at /content/drive/MyDrive/ImageNet_Images/preprocessed_splits/granularity/Time/all_channels/fine0/test_confusion.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(0); random.seed(0); np.random.seed(0)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,percent,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    percent = percent * 100\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "\n",
    "class RandomTimeMask:\n",
    "    \"\"\"Zero a contiguous time block of length  L  (in samples).\"\"\"\n",
    "    def __init__(self, p=0.5, L=15):\n",
    "        self.p, self.L = p, L\n",
    "    def __call__(self, x):                         # x: (1, C, T) or (B,1,C,T)\n",
    "        if random.random() > self.p: return x\n",
    "        _, C, T = x.shape\n",
    "        l = min(self.L, T-1)\n",
    "        t0 = random.randint(0, T-l-1)\n",
    "        x = x.clone()\n",
    "        x[:,:,t0:t0+l] = 0.\n",
    "        return x\n",
    "\n",
    "class RandomChannelDropout:\n",
    "    def __init__(self, p_channel=0.2, max_drop=4):\n",
    "        self.p, self.max_drop = p_channel, max_drop\n",
    "    def __call__(self, x):                         # x: (1,C,T)\n",
    "        if random.random() > self.p: return x\n",
    "        _, C, T = x.shape\n",
    "        k = random.randint(1, self.max_drop)\n",
    "        ch = torch.randperm(C, device=x.device)[:k]\n",
    "        x = x.clone()\n",
    "        x[:,ch,:] = 0.\n",
    "        return x\n",
    "\n",
    "class RandomTimeJitter:\n",
    "    def __init__(self, max_shift=8): self.max_shift=max_shift\n",
    "    def __call__(self, x):\n",
    "        if self.max_shift==0: return x\n",
    "        shift = random.randint(-self.max_shift, self.max_shift)\n",
    "        return torch.roll(x, shifts=shift, dims=-1)\n",
    "\n",
    "class RandomAmplitudeScale:\n",
    "    def __init__(self, min_s=0.9, max_s=1.1): self.a,self.b=min_s,max_s\n",
    "    def __call__(self, x):\n",
    "        s = torch.empty(1, device=x.device).uniform_(self.a, self.b)\n",
    "        return x * s\n",
    "\n",
    "class RandomGaussianNoise:\n",
    "    def __init__(self, std=0.03): self.std = std\n",
    "    def __call__(self, x):\n",
    "        return x + torch.randn_like(x) * self.std\n",
    "\n",
    "\n",
    "class EEGAugment:\n",
    "    \"\"\"Compose all small transforms\"\"\"\n",
    "    def __init__(self):\n",
    "        self.ops = [\n",
    "            RandomTimeJitter(12),\n",
    "            RandomChannelDropout(0.6, max_drop=8),\n",
    "            RandomTimeMask(0.8, L=25),\n",
    "            RandomAmplitudeScale(0.8,1.2),\n",
    "            RandomGaussianNoise(0.03),\n",
    "        ]\n",
    "    def __call__(self, x):\n",
    "        for op in self.ops:\n",
    "            x = op(x)\n",
    "        return x\n",
    "\n",
    "class EEGWindowDataset(Dataset):\n",
    "    def __init__(self, eeg_tensor, wnid_list,img,sub, class_to_idx, transform=None, fname=None):\n",
    "        self.X = eeg_tensor                     # (N,1,,n_freq_bins,time_window)\n",
    "        self.y = torch.tensor([class_to_idx[w] for w in wnid_list],\n",
    "                              dtype=torch.long)\n",
    "        self.transform = transform\n",
    "        self.image = img\n",
    "        self.subject = sub\n",
    "        self.fname = fname\n",
    "    def __len__(self): return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.X[idx], self.y[idx]\n",
    "\n",
    "        x = self.X[idx].float()\n",
    "\n",
    "        #     centre & scale — here we just scale to roughly [-1,1]\n",
    "        x = (x - x.mean()) / (x.std() + 1e-6)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y,self.image[idx], self.subject[idx],self.fname[idx]\n",
    "\n",
    "\n",
    "def latent_mixup(z, y, alpha=0.2):\n",
    "    \"\"\"z: (B, d)  – latent vectors\n",
    "       y: (B,)    – int labels\"\"\"\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    idx = torch.randperm(z.size(0), device=z.device)\n",
    "    z_mix = lam * z + (1 - lam) * z[idx]        # mixed latents\n",
    "    y_a, y_b = y, y[idx]\n",
    "    return z_mix, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def train_and_dump():\n",
    "    # ──────────────────────────────────────────────────────────\n",
    "    # 1) LOAD PREPROCESSED SPLITS FROM DISK (no more splitData)\n",
    "    # ──────────────────────────────────────────────────────────\n",
    "    channel = \"all_channels\"\n",
    "    granularity = \"fine0\"\n",
    "\n",
    "    split_dir = f\"/content/drive/MyDrive/ImageNet_Images/preprocessed_splits/granularity/Time/{channel}/{granularity}\"\n",
    "\n",
    "    # --- load *train* once --------------------------------------------------------\n",
    "    train_tensor, train_wnids, train_imgs, train_subj, train_fname = torch.load(f\"{split_dir}/train_timeNewHop32.pt\",\n",
    "                                              weights_only=False)\n",
    "\n",
    "    # build / read the mapping -----------------------------------------------------\n",
    "    json_path = f\"/content/drive/MyDrive/ImageNet_Images/preprocessed_splits/granularity/Time/{channel}/{granularity}/wnid_to_idx.json\"\n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path) as f:\n",
    "            class_to_idx = json.load(f)\n",
    "    else:\n",
    "        uniq = sorted(set(train_wnids))\n",
    "        class_to_idx = {w:i for i,w in enumerate(uniq)}\n",
    "        with open(json_path, \"w\") as f:\n",
    "            json.dump(class_to_idx, f, indent=2)\n",
    "\n",
    "    n_classes = len(class_to_idx)            # 80\n",
    "\n",
    "    # --- validation & test still come from disk (they're small) -------------------\n",
    "    val_tensor,  val_wnids,  val_imgs, val_subj, val_fname = torch.load(f\"{split_dir}/val_timeNewHop32.pt\",\n",
    "                                            weights_only=False)\n",
    "    test_tensor, test_wnids, test_imgs, test_subj, test_fname = torch.load(f\"{split_dir}/test_timeNewHop32.pt\",\n",
    "                                            weights_only=False)\n",
    "\n",
    "    # --- construct datasets *without* an extra disk read --------------------------\n",
    "    train_ds = EEGWindowDataset(train_tensor, train_wnids,train_imgs,train_subj,\n",
    "                                class_to_idx, transform=EEGAugment(),fname=train_fname)\n",
    "    val_ds   = EEGWindowDataset(val_tensor,  val_wnids, val_imgs, val_subj,\n",
    "                                class_to_idx, transform=None,fname=val_fname)\n",
    "    test_ds  = EEGWindowDataset(test_tensor, test_wnids, test_imgs, test_subj,\n",
    "                                class_to_idx, transform=None,fname=test_fname)\n",
    "\n",
    "    # ───────────────────────── 4. dataloaders ─────────────────────────────────────\n",
    "    batch_size = 256\n",
    "    loaders = dict(\n",
    "        train = DataLoader(train_ds, batch_size, shuffle=True,  num_workers=4),\n",
    "        val   = DataLoader(val_ds,   batch_size, shuffle=False, num_workers=4),\n",
    "        test  = DataLoader(test_ds,  batch_size, shuffle=False, num_workers=4),\n",
    "    )\n",
    "\n",
    "    # 0.001\n",
    "    # 3) MODEL, LOSS, optimiser, SCHEDULER\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = EEGEncoderCNN(n_classes,n_channels=62,hidden_dim=128).to(device)\n",
    "\n",
    "    def to_channels_last(t):\n",
    "        return t.to(device, memory_format=torch.channels_last)\n",
    "\n",
    "    model = model.to(memory_format=torch.channels_last)\n",
    "\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.15)\n",
    "    optimiser = AdamW(model.parameters(), lr=3e-4, weight_decay=1e-3)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimiser, T_max=150, eta_min=1e-6)\n",
    "    supcon = SupConLoss(temperature=0.07)\n",
    "\n",
    "    free_bit = 0.1\n",
    "    beta = 0.0\n",
    "    gamma = 0.0\n",
    "\n",
    "    epochs_no_improve = 0\n",
    "    num_epochs   = 150\n",
    "    best_val_acc = 0.0\n",
    "    save_path    = f\"/content/drive/MyDrive/ImageNet_Images/preprocessed_splits/granularity/Time/{channel}/{granularity}/best_eeg_cnn.pt\"\n",
    "    warmup = 10\n",
    "    ramp = 30\n",
    "    patience = 15\n",
    "    early_stop_start = warmup + ramp + patience\n",
    "    beta_max  = 0.0003\n",
    "    gamma_max = 5e-5\n",
    "    max_logvar = 1.5\n",
    "    free_bit = 0.05\n",
    "    # 4) TRAIN + VALIDATION LOOP\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for phase in (\"train\", \"val\"):\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            epoch_loss = 0.0\n",
    "            correct    = 0\n",
    "            total      = 0\n",
    "            sum_ce, sum_kl, sum_supcon, sum_total, total_samples = 0, 0, 0, 0, 0\n",
    "            if epoch < warmup:\n",
    "                beta = 0.0\n",
    "            elif epoch < warmup+ramp:\n",
    "                beta = beta_max * (epoch - warmup) / ramp\n",
    "            else:\n",
    "                beta = beta_max\n",
    "\n",
    "            gamma = gamma_max * (beta / beta_max)\n",
    "            # ---- on the very first training epoch ----\n",
    "            if epoch == warmup:\n",
    "                for m in [model.conv1, model.conv2, model.conv3,\n",
    "                          model.conv4, model.res_conv, model.norm_res]:\n",
    "                    for p in m.parameters():\n",
    "                        p.requires_grad = False\n",
    "                    m.eval()\n",
    "            # ---- unfreeze when KL ramp is finished ----\n",
    "            if epoch == warmup+ramp:\n",
    "                for m in [model.conv1, model.conv2, model.conv3,\n",
    "                          model.conv4, model.res_conv, model.norm_res]:\n",
    "                    for p in m.parameters():\n",
    "                        p.requires_grad = True\n",
    "                    m.train()\n",
    "\n",
    "\n",
    "            for X, y,_,_,_ in loaders[phase]:\n",
    "                # X: shape (B, 1, 4, 100)\n",
    "                # y: shape (B,)\n",
    "                B, C, W, T = X.shape  # C = 1, W = 4, T = 100\n",
    "                X = to_channels_last(X)       # (B,1,4,100)\n",
    "                y = y.to(device)      # (B,)\n",
    "                # y: (B,)\n",
    "\n",
    "                # 1) collapse “channels” into batch (but here C=1, so this is effectively a no-op)\n",
    "                #    The original code did X.view(B*C,1,W,T), but since C=1:\n",
    "                Xf = X.view(B * C, 1, W, T)  # → actually still (B,1,4,100)\n",
    "\n",
    "                # 2) forward through shared encoder\n",
    "                lat_f, mu, logvar, _ = model(Xf.to(device,memory_format=torch.channels_last))\n",
    "\n",
    "                # 3) reshape & (the original did a 5-window concat, but we have only 1 “chunk”)\n",
    "                #    In the old code they assumed 5 windows per trial (so C=5). Here C=1, so:\n",
    "                lat = lat_f.view(B, C, -1).reshape(B, 128 * C)  # → (B, 128)\n",
    "\n",
    "                # clamp logvar\n",
    "                logvar = torch.clamp(logvar, max=max_logvar)\n",
    "\n",
    "                if phase == \"train\" and epoch>early_stop_start:                             # ← MIXUP ONLY DURING TRAIN\n",
    "                    z, y_a, y_b, lam = latent_mixup(mu, y, alpha=0.1)   # ★\n",
    "                    logits = model.classifier(z)                         # ★\n",
    "                    loss_ce = (lam * criterion(logits, y_a)              # ★\n",
    "                              + (1 - lam) * criterion(logits, y_b))\n",
    "                else:\n",
    "                    logits  = model.classifier(mu)\n",
    "                    loss_ce = criterion(logits, y)\n",
    "                # compute per‐sample, per‐dim KL\n",
    "                # free‐bits: clamp each dimension’s KL to at least free_bit\n",
    "                # sum over dims → (B,) then mean over batch → scalar\n",
    "                kl_per_dim = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp())  # (B, hidden_dim)\n",
    "                kl = torch.clamp(kl_per_dim, min=free_bit).sum(dim=1).mean()\n",
    "\n",
    "                contrastive_loss = supcon(mu, y)\n",
    "\n",
    "                loss = loss_ce + beta * kl + gamma * contrastive_loss\n",
    "\n",
    "                if phase == \"train\":\n",
    "                    optimiser.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimiser.step()\n",
    "\n",
    "                epoch_loss += loss.item() * B\n",
    "                sum_ce      += loss_ce.item()      * B\n",
    "                sum_kl      += kl.item()      * B\n",
    "                sum_supcon  += contrastive_loss.item()  * B\n",
    "\n",
    "                preds       = logits.argmax(dim=1)\n",
    "                correct    += (preds == y).sum().item()\n",
    "                total      += B\n",
    "\n",
    "            epoch_loss /= total\n",
    "            epoch_acc   = correct / total\n",
    "            avg_ce      = sum_ce     / total\n",
    "            avg_kl      = sum_kl     / total\n",
    "            avg_supcon  = sum_supcon / total\n",
    "            print(f\"{phase.title()} Epoch {epoch:03d} | \"\n",
    "                  f\"Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} \"\n",
    "                  f\"CE: {avg_ce:.4f}  KL: {avg_kl:.4f}  SupCon: {avg_supcon:.4f}  \")\n",
    "\n",
    "            if phase == \"val\" and epoch_acc > best_val_acc:\n",
    "                best_val_acc = epoch_acc\n",
    "                epochs_no_improve = 0\n",
    "                ckpt = {\n",
    "                    'model'     : model.state_dict(),\n",
    "                    'optimiser' : optimiser.state_dict(),\n",
    "                    'scheduler' : scheduler.state_dict(),\n",
    "                    'epoch'     : epoch,\n",
    "                    'best_acc'  : best_val_acc,\n",
    "                }\n",
    "                torch.save(ckpt, save_path)\n",
    "            elif phase == \"val\" and epoch>=early_stop_start:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience and epoch>early_stop_start:\n",
    "            print(f\"No improvement in {patience} epochs, stopping early.\")\n",
    "            break\n",
    "        scheduler.step()\n",
    "\n",
    "    # ─── TEST PHASE ──────────────────────────────────────────────\n",
    "\n",
    "    ckpt = torch.load(save_path)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    epoch_start     = ckpt['epoch'] + 1\n",
    "    optimiser.load_state_dict(ckpt['optimiser'])\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    test_correct = 0\n",
    "    test_total   = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y,_,_,_ in loaders[\"test\"]:\n",
    "            X = X.to(device)                         # (B,1,62,100)\n",
    "            y = y.to(device)\n",
    "            B, C, W, T = X.shape                    # here C == 1\n",
    "            _, mu, _, _ = model(X.view(B*C,1,W,T))\n",
    "\n",
    "            # quick sanity-check accuracy\n",
    "            logits  = model.classifier(mu)\n",
    "\n",
    "            preds  = logits.argmax(dim=1)\n",
    "            test_correct += (preds == y).sum().item()\n",
    "            test_total   += B\n",
    "\n",
    "    print(f\"Test Acc: {test_correct/test_total:.4f}\")\n",
    "\n",
    "    # ─── DUMP LATENTS FOR EACH SPLIT ─────────────────────────────\n",
    "    for split in (\"train\", \"val\", \"test\"):\n",
    "        all_lat, all_lbl,all_img, all_sub, all_preds, all_fnames = [], [], [],[],[],[]\n",
    "        for X, y,image,subject, fname in loaders[split]:\n",
    "\n",
    "            X = X.to(device)                         # (B,1,62,100)\n",
    "            y = y.to(device)\n",
    "            B, C, W, T = X.shape                    # here C == 1\n",
    "            _, mu, _, _ = model(X.view(B*C,1,W,T))\n",
    "\n",
    "            # quick sanity-check accuracy\n",
    "            logits  = model.classifier(mu)\n",
    "\n",
    "            lat = mu.cpu().detach().numpy()                      # (B, 128) — no extra reshape\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "\n",
    "            all_pred_batch = preds.cpu().numpy()\n",
    "            all_preds.append(all_pred_batch)\n",
    "\n",
    "            all_lat.append(lat)\n",
    "            all_lbl.append(y.cpu().numpy())\n",
    "            all_img.append(image)\n",
    "            all_sub.append(subject.cpu().numpy())\n",
    "            all_fnames.extend(fname)\n",
    "\n",
    "        L = np.concatenate(all_lat, axis=0)  # → (N, 1, 128)\n",
    "        Y = np.concatenate(all_lbl, axis=0)  # → (N,)\n",
    "        I = np.concatenate(all_img,axis=0)\n",
    "        S = np.concatenate(all_sub,axis=0)\n",
    "        P = np.concatenate(all_preds,axis=0)\n",
    "        F = np.array(all_fnames,dtype=object)\n",
    "\n",
    "        # Save as .npy\n",
    "        latent_savePath = f\"/content/drive/MyDrive/ImageNet_Images/latent_dumps/granularity/Time/{channel}/{granularity}/\"\n",
    "\n",
    "        if not os.path.exists(latent_savePath):\n",
    "          os.makedirs(latent_savePath)\n",
    "\n",
    "        np.save(f\"{latent_savePath}{split}_latentsNew.npy\", L)\n",
    "        np.save(f\"{latent_savePath}{split}_labelsNew.npy\",  Y)\n",
    "        np.save(f\"{latent_savePath}{split}_imagesNew.npy\", I)\n",
    "        np.save(f\"{latent_savePath}{split}_subjectNew.npy\",  S)\n",
    "        np.save(f\"{latent_savePath}{split}_filenamesNew.npy\",F)\n",
    "               # 2) Compute confusion matrix\n",
    "        cm = confusion_matrix(Y, P, labels=list(class_to_idx.values()))\n",
    "\n",
    "        # 3) Plot & save\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plot_confusion_matrix(cm,\n",
    "                              classes=[k for k,v in sorted(class_to_idx.items(), key=lambda x: x[1])],\n",
    "                              percent=False,\n",
    "                              title=f\"{split} confusion\")\n",
    "        fn = os.path.join(split_dir, f\"{split}_confusion.png\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(fn)\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"Dumped {split}: latents {L.shape}, labels {Y.shape}, images {I.shape}, subjects {S.shape}, CM at {fn}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    freeze_support()\n",
    "    train_and_dump()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOccENfkITI11Dxt2S9DBmW",
   "gpuType": "L4",
   "machine_shape": "hm",
   "mount_file_id": "1ZS0un4M2VnVWxHK80noXZucjRlQRijSv",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
